# Narrative-Protocol


Architektur und Implementierung eines KI-nativen Narrativ-IDEs: Entwicklung eines Roman-Schreibassistenten auf Vercel mit TypeScript1. Exekutive Zusammenfassung: Der Paradigmenwechsel zum Agentischen SchreibenDie Softwareentwicklung hat durch KI-gestützte Assistenten wie Cursor, Kiro und Roo Code eine radikale Transformation erfahren. Diese Werkzeuge fungieren nicht mehr als bloße Autovervollständiger, sondern als intelligente Partner, die den semantischen Kontext einer Codebasis verstehen, Hintergrundprozesse verwalten und mehrstufige logische Operationen durchführen. Das Domänenfeld des kreativen Schreibens, insbesondere komplexe Roman-Projekte, steht vor einer analogen Revolution. Die Anforderungen an das Schreiben eines Romans – das Management tausender verknüpfter Entitäten (Charaktere), temporaler Abhängigkeiten (Handlungsstränge) und struktureller Regeln (Dramaturgie) – sind in ihrer Komplexität mit der Architektur großer Softwareprojekte vergleichbar.Dieser Bericht entwirft eine umfassende technische Blaupause für eine KI-native Schreibumgebung, die als "Cursor für Romanautoren" konzipiert ist. Im Gegensatz zu herkömmlichen Chatbots operiert dieses System über ein Chat-First-Interface, das von autonomen Agenten unterstützt wird, die im Hintergrund eine persistente Kenntnis des Manuskriptzustands pflegen. Unter Verwendung des Vercel AI SDK für Streaming-Interfaces, OpenRouter für die Modell-Orchestrierung und TypeScript für eine typsichere Infrastruktur wird eine Plattform definiert, die narratives Reasoning auf höchstem Niveau ermöglicht.Ein zentrales Element dieser Architektur ist der Übergang von einfacher Retrieval-Augmented Generation (RAG) zu GraphRAG, welches das Narrativ nicht als bloße Textfragmente, sondern als Wissensgraph aus Beziehungen und Ereignissen modelliert. Um die explizite Anforderung der "kritischen Hinterfragung" zu erfüllen, implementiert das System Reflexion-Patterns und Constitutional AI-Leitplanken. Dies befähigt die KI, nicht nur als Verfasser, sondern als aktiver Lektor zu agieren, der Plot-Holes identifiziert, tonale Inkonsistenzen aufzeigt und die dramaturgische Integrität wahrt.Die vorliegende Analyse bietet einen erschöpfenden technischen Leitfaden für den Aufbau dieses Systems, der den gesamten Stack vom Next.js-Frontend bis zur LangGraph-Orchestrierungsschicht abdeckt und eine robuste, skalierbare und kosteneffiziente Lösung auf der serverlosen Infrastruktur von Vercel gewährleistet.2. Das Fundament der Architektur: Der Narrative IDE StackDie Entwicklung einer fortschrittlichen KI-Anwendung auf Vercel erfordert eine präzise Navigation durch die Kompromisse zwischen zustandslosen Serverless Functions und den zustandsbehafteten Anforderungen agentischer Workflows. Die vorgeschlagene Architektur integriert das Vercel-Ökosystem mit Mustern der dauerhaften Ausführung (Durable Execution), um das für einen Roman erforderliche langfristige Kontextmanagement zu bewältigen.2.1 Kerninfrastruktur: Vercel AI SDK und Next.jsDie Anwendung basiert auf Next.js (App Router) unter Verwendung von TypeScript, was ein einheitliches Framework für sowohl die Frontend-UI als auch die Backend-API-Routen bietet. Das Vercel AI SDK dient als primäre Schnittstelle zwischen der Anwendungslogik und den Large Language Models (LLMs) via OpenRouter.2.1.1 Vercel AI SDK Core als RückgratDas Vercel AI SDK Core 1 stellt die essenziellen Funktionen generateText und streamText bereit, die die Unterschiede zwischen verschiedenen Modellprovidern abstrahieren. Für einen Roman-Schreibassistenten ist die "Tool Calling"-Fähigkeit von überragender Bedeutung. Das System generiert nicht lediglich Text; es ruft Werkzeuge auf, um Kapitel zu lesen, Charakterbögen zu aktualisieren oder die Zeitlinie zu durchsuchen.Die Funktion streamUI (weiterentwickelt zu streamObject und Komponentengenerierung) ermöglicht es der Anwendung, React-Komponenten direkt innerhalb des Chat-Streams zu rendern. Wenn ein Nutzer beispielsweise den Befehl gibt: "Erstelle einen neuen Antagonisten", gibt der Agent keine Textbeschreibung aus, sondern streamt eine strukturierte Komponente, die ein interaktives Formular für das Charakterblatt rendert.1 Dies entspricht der UX moderner Coding-Assistenten, bei denen KI-Vorschläge direkt in die Arbeitsumgebung integriert werden.2.1.2 OpenRouter als strategische MiddlewareDer Einsatz von OpenRouter fungiert als strategische Middleware-Schicht. Er ermöglicht dem Nutzer (oder dem Routing-Agenten), dynamisch zwischen Modellen zu wechseln, die für spezifische Aufgaben am besten geeignet sind:Claude 3.5 Sonnet für nuancierte Prosa-Generierung und stilistische Analysen.GPT-4o für rigorose Logikprüfungen und Konsistenzchecks.Llama 3 oder Haiku für volumenintensive Zusammenfassungen im Hintergrund.1Das Vercel AI SDK unterstützt OpenRouter als OpenAI-kompatiblen Provider, was eine nahtlose Integration ermöglicht:TypeScriptimport { createOpenAI } from '@ai-sdk/openai';

// Konfiguration der OpenRouter-Instanz
const openrouter = createOpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: process.env.OPENROUTER_API_KEY,
});

// Implementierung im Vercel AI SDK
const result = await streamText({
  model: openrouter('anthropic/claude-3.5-sonnet'),
  system: 'Du bist ein erfahrener Lektor für belletristische Literatur...',
  prompt: 'Analysiere den Spannungsbogen von Kapitel 3...',
});
Diese Flexibilität ist entscheidend, um die Kosten zu optimieren und gleichzeitig die höchste Qualität für kritische Aufgaben zu gewährleisten.32.2 Management zustandsbehafteter Agenten: LangGraph.js vs. Vercel AI SDK CoreEin kritischer Entscheidungspunkt ist die Wahl des Orchestrierungs-Frameworks für die Agenten. Die Anforderung, dass die App "wie Kiro oder Cursor" arbeiten soll, impliziert Hintergrund-Kontextmanagement und mehrstufiges Reasoning, das über einfache Frage-Antwort-Zyklen hinausgeht.2.2.1 Das Argument für LangGraph.jsLangGraph.js 4 glänzt bei komplexen, zustandsbehafteten Workflows. Es modelliert den Agenten als Graphen aus Knoten (Schritten) und Kanten (Übergängen). Dies ist essenziell für das Feature der "Kritischen Hinterfragung". Ein Kritik-Agent muss folgenden zyklischen Prozess durchlaufen:Lesen des Entwurfs.Kritisieren anhand eines Regelwerks (Constitutional AI).Entscheiden, ob die Kritik valide und substanziell ist.Wenn valide: Änderungen vorschlagen; wenn nicht: Reflexion und erneuter Versuch.Dieses zyklische Verhalten (Loops) ist mit einer rein linearen Kette (Chain) schwer zu implementieren. Die Zustandspersistenz von LangGraph ermöglicht es dem Agenten zudem, zu pausieren, auf Nutzerfeedback zu warten und an genau diesem Punkt wieder aufzusetzen – ein Schlüsselfeature für einen "Human-in-the-Loop"-Schreibassistenten.52.2.2 Das Argument für Vercel AI SDK CoreAlternativ hat Vercel AI SDK Core 4 leistungsstarke Muster für mehrstufiges Tool-Calling eingeführt. Es automatisiert die Schleife von "Tool aufrufen -> Ergebnis erhalten -> LLM aufrufen -> Tool aufrufen". Für Standardaufgaben wie "Durchsuche die Lore-Datenbank und schreibe eine Szene" ist das Vercel AI SDK leichtgewichtiger und integriert sich besser in die Streaming-Response-Patterns von Next.js.Architektur-Empfehlung: Ein Hybrid-Ansatz.Verwendung von Vercel AI SDK Core für das Echtzeit-Chat-Interface (geringe Latenz, Text-Streaming) und einfache Tool-Nutzung.Verwendung von LangGraph.js (laufend auf einem dauerhaften Backend oder via Inngest) für komplexe Hintergrundaufgaben wie "Analysiere das gesamte Manuskript auf Plot-Holes" oder "Entwickle einen alternativen Handlungsstrang über 10 Kapitel".52.3 Lösung des "Hintergrundarbeit"-Problems auf ServerlessVercel Serverless Functions haben strikte Ausführungszeitlimits (typischerweise 10–60 Sekunden in Hobby/Pro-Plänen, bis zu 900s in Enterprise).6 Ein "Cursor-ähnliches" Erlebnis erfordert jedoch Agenten, die im Hintergrund arbeiten – Tausende von Wörtern indizieren, Kapitel erneut lesen oder den Wissensgraphen aktualisieren –, was diese Limits häufig überschreitet.2.3.1 Durable Execution: Inngest vs. Trigger.devUm langlaufende narrative Analysen zu unterstützen, ohne in Timeouts zu laufen, muss die Architektur diese Aufgaben an eine Engine für Durable Execution auslagern. Inngest 7 und Trigger.dev 8 sind hier die führenden Lösungen für das TypeScript-Ökosystem.Inngest: Funktioniert, indem Events an Vercel-Funktionen gesendet werden, der Zustand und die Wiederholungsversuche (Retries) jedoch extern verwaltet werden. Es ermöglicht "Step Functions", bei denen ein Agent stundenlang laufen, schlafen oder auf externe Events warten kann. Es ist "Serverless-Native" und integriert sich tief in Next.js-Patterns.7Trigger.dev: Bietet ähnliche Fähigkeiten, konzentriert sich jedoch stark auf langlaufende Jobs, die möglicherweise vollständig außerhalb der Vercel-Function-Lambda (in eigener Infrastruktur) laufen müssen, während sie dennoch im Codebase definiert sind.8Entscheidung: Inngest wird für dieses Projekt empfohlen, da sein ereignisgesteuertes Modell perfekt zum "Chat-Interface"-Paradigma passt. Wenn ein Nutzer den Agenten bittet: "Lies alle 50 Kapitel und prüfe die Konsistenz der Augenfarben aller Charaktere", pusht das Chat-Interface ein Event an Inngest. Die Inngest-Funktion (der Hintergrund-Agent) führt die Arbeit schrittweise aus (step.run), aktualisiert den Fortschritt in der Datenbank und pusht Status-Updates über WebSockets zurück an die UI, wodurch die Vercel-Timeouts umgangen werden, während die Infrastruktur einfach bleibt.103. Das Datenmodell "Roman-Projekt": Kontext-ManagementDas definierende Merkmal einer IDE für Romanautoren ist die Art und Weise, wie sie Kontext verwaltet. Im Gegensatz zu Code, der eine strikte Syntax (Importe, Definitionen) aufweist, ist narrativer Kontext semantisch, implizit und über das gesamte Werk verteilt. "Kontext-Management" bedeutet hier ein System, das die richtigen narrativen Details zum richtigen Zeitpunkt abruft und bewahrt.3.1 Hierarchische Datenstruktur und JSON SchemaDie Standard-Dateistruktur (Flat Files) ist für komplexe Romane unzureichend. Das Datenmodell muss die Hierarchie eines Romans widerspiegeln:Projekt (Roman): Der Root-Container.Akte/Teile: Dramaturgische Grobcontainer.Kapitel: Die primären Kompositionseinheiten.Szenen: Die atomaren Einheiten der Handlung.Entitäten: Charaktere, Orte, Objekte (Lore).Diese Struktur sollte mittels JSON Schema 11 definiert werden, um sicherzustellen, dass die Agenten mit strukturierten Daten interagieren und valide Updates durchführen.EbeneBeschreibungDatentypBeispielhafte AttributeRomanWurzelobjektDokumentTitel, Genre, Zielgruppe, MetadatenAktGrobstrukturContainerAkt-Nummer, Thematischer Fokus, Spannungsbogen-ZielKapitelKompositionContainerSequenznummer, Titel, POV-Charakter, WortzahlSzeneHandlungseinheitTextblockZeitstempel (In-World), Ort, Anwesende Charaktere, StimmungEntitätWissensbausteinNodeName, Typ (Person/Ort), Attribute (JSON), BeziehungenEin Beispiel für ein JSON-Schema einer Szene:JSON{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Scene",
  "type": "object",
  "properties": {
    "id": { "type": "string", "format": "uuid" },
    "pov_character": { "type": "string", "description": "ID des Point-of-View Charakters" },
    "setting": { "type": "string", "description": "ID des Handlungsortes" },
    "timestamp": { "type": "string", "description": "Chronologische Zeit innerhalb der Welt" },
    "content": { "type": "string", "description": "Der eigentliche Prosatext" },
    "summary": { "type": "string", "description": "LLM-generierte hierarchische Zusammenfassung" },
    "sentiment": { "type": "string", "enum": }
  }
}
Diese strukturierte Ablage ermöglicht es Agenten, gezielte Abfragen zu stellen, wie z.B. "Liste alle Szenen auf, in denen der Protagonist in 'Berlin' ist und die Stimmung 'Negativ' ist".3.2 GraphRAG: Die Wissensmaschine für Narrative KausalitätKlassisches RAG (Vector Search) ist für Romane oft unzureichend. Wenn ein Nutzer fragt: "Wie hat sich die Beziehung des Protagonisten zu seinem Vater verändert?", liefert die Vektorsuche möglicherweise Szenen zurück, in denen beide interagieren, kann aber nicht über die Trajektorie der Beziehung "nachdenken" oder Kausalitäten erkennen. GraphRAG (Graph Retrieval-Augmented Generation) 13 löst dieses Problem, indem es Vektor-Embeddings mit einem Wissensgraphen kombiniert.3.2.1 Implementierung von GraphRAGDas System konstruiert einen Graphen, in dem:Knoten (Nodes) Entitäten (Charaktere, Orte) und Plot-Ereignisse darstellen.Kanten (Edges) Beziehungen sind (z.B. LIEBT, VERRÄT, BEFINDET_SICH_IN, FÜHRT_ZU).Wenn der Nutzer das System abfragt, traversiert der Agent diesen Graphen. Um Inkonsistenzen zu finden, verfolgt er beispielsweise die BEFINDET_SICH_IN-Kanten eines Charakters über die Zeit, um sicherzustellen, dass dieser nicht an zwei Orten gleichzeitig ist.Technische Umsetzung:Datenbank: Neo4j ist der Industriestandard für Graphdatenbanken und bietet exzellente Integrationen für GraphRAG.15 Alternativ bietet sich SurrealDB als Multi-Model-Datenbank an, die Dokumente und Graphen in einem System vereint, was die Komplexität des Stacks reduziert.Extraktions-Agent: Ein Hintergrund-Agent (via Inngest) verarbeitet jede neu geschriebene Szene. Er nutzt ein LLM, um Triplets (Subjekt -> Prädikat -> Objekt) zu extrahieren und den Graphen zu aktualisieren.17Input Text: "Alice schlug die Tür vor Bob zu und beendete damit ihre Partnerschaft endgültig."Graph Update: Erstelle/Update Kante Alice --> Bob.3.3 Hierarchische Zusammenfassung und Long Context ManagementObwohl Modelle wie Claude 3.5 Kontextfenster von 200k+ Token bieten, ist es kostenintensiv und langsam, einen gesamten 100k-Wörter-Roman in jeden Prompt zu laden (Latenz steigt mit Kontextlänge).19 Zudem leiden Modelle bei extrem langen Kontexten oft unter dem "Lost-in-the-Middle"-Phänomen, bei dem Informationen in der Mitte des Prompts ignoriert werden.Die Architektur setzt daher auf Hierarchische Zusammenfassung (Tree of Summaries).21Level 1 (Szene): Der Agent generiert eine dichte Zusammenfassung (unter Verwendung von Chain of Density Prompting) für jede Szene.23 Dies stellt sicher, dass keine kritischen Entitäten verloren gehen.Level 2 (Kapitel): Szenenzusammenfassungen werden zu einer Kapitelzusammenfassung aggregiert.Level 3 (Akt/Gesamtwerk): Kapitelzusammenfassungen werden zu einer Akt-Zusammenfassung verdichtet.Strategie: Wenn der Nutzer eine globale Frage stellt ("Ist das Pacing im zweiten Akt zu langsam?"), lädt der Agent nur die Level-2- oder Level-3-Zusammenfassungen in den Kontext. Dies reduziert die Token-Anzahl massiv und ermöglicht schnelles Reasoning über die gesamte Struktur. Fragt der Nutzer nach einem Detail ("Welche Farbe hatten die Vorhänge in Kapitel 1?"), nutzt das System RAG, um den spezifischen Szenentext zu laden.244. Agentische Workflows und die Skills-SchnittstelleDer Nutzer spezifizierte eine "Skills-Schnittstelle". In der Architektur des Vercel AI SDK bildet dies auf Tools und das Model Context Protocol (MCP) ab.25 Diese Skills sind die handlungsfähigen Verb-Komponenten des Systems.4.1 Definition von Skills als ToolsSkills sind diskrete Fähigkeiten, die dem LLM exponiert werden. In TypeScript werden diese mittels Zod-Schemata für absolute Typsicherheit definiert.64.1.1 Kern-Skills (System Tools)Diese Tools sind fest in das Backend integriert:read_manuscript: Erlaubt dem Agenten, Text via semantischer Suche oder Graph-Traversal abzufragen.write_scene: Hängt Text an eine spezifische Szenen-ID an oder modifiziert diesen.update_lore: Modifiziert die Entitäts-Datenbank (z.B. Ändern der Augenfarbe eines Charakters im "Character Sheet").scan_consistency: Triggers einen Hintergrund-Check via Inngest auf Logikfehler.4.1.2 Das Model Context Protocol (MCP)Vercel hat das Model Context Protocol 25 adaptiert. Dies ist entscheidend für die "Cursor-ähnliche" Erfahrung. MCP ermöglicht es dem Schreibassistenten, sich mit externen Datenquellen zu verbinden, ohne dass diese manuell kopiert werden müssen. Wenn der Nutzer seine Notizen in Notion oder Obsidian speichert, kann ein MCP-Server diese Daten in den Kontext des Schreibassistenten brücken. Dies öffnet die Architektur für ein Ökosystem von Plugins.4.2 Multi-Agent Orchestrierungs-MusterUm komplexe Verhaltensweisen zu realisieren, nutzen wir spezifische Agenten-Orchestrierungs-Muster 27:4.2.1 Das Planer-Exekutor-Muster (Planner-Executor)Für eine Anfrage wie "Schreibe Kapitel 5 um, damit es spannender wird", scheitert ein einzelner LLM-Aufruf oft an der Konsistenz.Planer-Agent: Analysiert Kapitel 5 und skizziert die notwendigen Änderungen für "Spannung" (z.B. "Verzögere die Enthüllung der Waffe", "Verkürze die Satzstrukturen").Exekutor-Agent: Nimmt den Plan und den Originaltext und wendet die Änderungen Sektion für Sektion an.Verifizierer-Agent: Liest den neuen Text, um sicherzustellen, dass er dem Plan entspricht und keine neuen Fehler eingeführt hat.Dieser Fluss wird über LangGraph orchestriert, wobei der Zustand (State) zwischen den Knoten weitergereicht wird.44.2.2 Das Swarm-Muster (Kollaborative Agenten)Für das World-Building ist ein Swarm-Muster 29 effektiv. Mehrere spezialisierte Agenten brainstormen gemeinsam.Nutzer: "Entwickle ein Magiesystem basierend auf Schall."Historiker-Agent: Schlägt vor, wie alte Mythen Schall betrachteten.Geograph-Agent: Schlägt vor, wie Akustik den Städtebau beeinflusst.Controller-Agent: Synthetisiert diese Inputs zu einem kohärenten Systemeintrag in der Enzyklopädie.295. Kritische Hinterfragung (Critical Questioning) & ReflexionEin Schlüsselfeature der Anforderung ist die "Kritische Hinterfragung". Diese Fähigkeit unterscheidet einen Schreibpartner von einem sycophantischen Chatbot. Standard-LLMs sind durch RLHF (Reinforcement Learning from Human Feedback) darauf trainiert, hilfreich und zustimmend zu sein, und zögern oft, harte, notwendige Kritik zu üben.305.1 Constitutional AI und System PromptsUm echte Kritik zu ermöglichen, müssen wir den "Zustimmungs-Bias" mittels Constitutional AI-Prinzipien überschreiben.31 Der System-Prompt für die "Kritiker"-Persona enthält spezifische Direktiven, die als "Verfassung" des Agenten dienen:"Du bist der 'Advocatus Diaboli' dieses Manuskripts. Dein Ziel ist NICHT, dem Nutzer zu gefallen, sondern die Geschichte zu stärken. Du musst dich strikt an diese Prinzipien halten:Kohärenz: Melde jede Verletzung der Zeitlinie oder Charakterlogik sofort.Show, Don't Tell: Identifiziere rücksichtslos Exposition-Dumps.Negativitäts-Bias: Suche explizit nach Schwächen, nicht nach Stärken.Sei nicht höflich. Sei präzise."5.2 Das Reflexion-PatternWir implementieren das Reflexion-Pattern 33, um die Qualität der Kritik zu steigern.Entwurf (Draft): Der Agent generiert eine erste Kritik.Reflexion (Reflect): Der Agent (oder ein separater "Supervisor"-Modell) analysiert seine eigene Kritik. Prompt: "Ist diese Kritik spezifisch genug? Wurde der Widerspruch in Kapitel 2 übersehen? Ist sie zu vage?"Verfeinerung (Refine): Der Agent schreibt die Kritik basierend auf dieser Reflexion um.Dieser Loop stellt sicher, dass der Nutzer hochwertiges, umsetzbares Feedback erhält, statt generischer Ratschläge wie "Zeigen, nicht erzählen".5.3 Automatisierte Konsistenzprüfung (Plot Hole Detection)Dies ist ein spezifischer "Skill", der durch den Wissensgraphen angetrieben wird.Temporale Konsistenz: Der Agent prüft Zeitstempel an Szenen-Knoten. Wenn Szene A (Montag) Szene B (Dienstag) verursacht, aber Szene B ein Objekt benötigt, das erst in Szene C (Mittwoch) gefunden wird, identifiziert die Graphtraversierung den Zyklus oder die Unmöglichkeit.34Charakterstimmen-Konsistenz: Ein Hintergrundjob sampelt Dialoge eines Charakters über das gesamte Buch und nutzt ein LLM, um die "Stimme/Tonfall" zu klassifizieren. Wenn der Dialog in Kapitel 10 signifikant vom etablierten Cluster abweicht, wird eine Warnung markiert.366. User Interface: Die Chat-First IDEDie Benutzeroberfläche muss Textbearbeitung nahtlos mit Chat verbinden. Wir nutzen Next.js und Tailwind CSS, unter Verwendung von Komponenten aus shadcn/ui und assistant-ui für ein professionelles Look-and-Feel.6.1 Das "Cursor"-LayoutDas Interface imitiert eine IDE, um Fokus und Produktivität zu maximieren:Zentraler Bereich: Der Manuskript-Editor (Rich Text oder Markdown). Wir können Bibliotheken wie TipTap oder Slate.js verwenden, die sich leicht in JSON serialisieren lassen, damit Agenten sie lesen können.12Rechter Bereich: Das Chat-Interface (Vercel AI SDK). Hier interagiert der Nutzer mit den Agenten.Linker Bereich: Der "Projekt-Explorer" (Roman-Projekt-Struktur). Statt Dateien zeigt er Akte, Kapitel und Charakterblätter an.6.2 Assistant UI IntegrationAssistant-ui 37 ist eine React-Bibliothek, die speziell für den Bau von AI-Chat-Interfaces entwickelt wurde. Sie integriert sich nativ mit dem Vercel AI SDK und ermöglicht:Thread History: Persistente Chat-Threads, die mit spezifischen Kapiteln verknüpft sind.Generative UI: Wenn der Agent einen Plot-Outline vorschlägt, rendert dieser als klickbare, editierbare Timeline-Komponente, nicht nur als Text.Optimistic Updates: Die UI fühlt sich reaktionsschnell an, auch wenn der Agent im Hintergrund "denkt".6.3 Visualisierung von HintergrundstatusDa Agenten im Hintergrund arbeiten (via Inngest), benötigt die UI einen Weg, Fortschritt anzuzeigen. Wir implementieren einen "Background Tasks"-Indikator (ähnlich dem Build-Spinner in einer IDE).Status: "Agent 'Continuity-Check' liest Kapitel 4..."Notification: "Plot Hole in Kapitel 7 entdeckt."Dies wird mittels Server Sent Events (SSE) oder spezifischen Hooks im Vercel AI SDK erreicht, um Status-Updates vom Backend-Prozess zu streamen.257. Implementierungs-RoadmapPhase 1: Das Fundament (Core Chat & Editor)Ziel: Funktionale Next.js App auf Vercel, in der Nutzer schreiben und mit einem LLM darüber chatten können.Stack: Next.js, Vercel AI SDK, OpenRouter (Claude 3.5 Sonnet), PostgreSQL (für Textspeicherung).Feature: Basic RAG (Vector Search), um dem Chat Sichtbarkeit auf den Text zu geben.Phase 2: Die Agentische Schicht (Tiefer Kontext)Ziel: Hintergrundverarbeitung und strukturierte Daten.Stack: Inngest (für langlaufende Tasks), Neo4j (für Knowledge Graph).Feature: "Roman-Projekt"-Schema-Implementierung. Agenten fassen Szenen automatisch zusammen und extrahieren Entitäten in den Graphen.Phase 3: Der Kritiker & Skills (Advanced Workflow)Ziel: Proaktive Assistenz.Stack: LangGraph (für Kritik-Loops).Feature: Implementierung der "Critic"-Persona mit Constitutional AI. Reflexion-Loops für tiefe Analyse. "Skills"-Menü erlaubt Nutzern, spezifische Checks auszulösen ("Prüfe Pacing", "Analysiere Charakterbogen").8. Detaillierte Technische Analyse8.1 Datenpersistenz und SicherheitFür ein "Roman-Projekt" ist Datenverlust inakzeptabel.Datenbank: PostgreSQL (via Supabase oder Neon) ist optimal für den Manuskripttext und JSON-Strukturen. Es ist zuverlässig, relational und unterstützt Vektor-Erweiterungen (pgvector) für das RAG-System.Auth: Clerk oder Auth0 für sicheres Nutzermanagement.Verschlüsselung: Da Romane geistiges Eigentum sind, sollte sichergestellt werden, dass alle Texte "at rest" verschlüsselt sind.8.2 KostenoptimierungsstrategieLLM-Aufrufe für einen ganzen Roman sind teuer.Caching: Nutzung des KV-Caches in OpenRouter/LLM-Providern, um das erneute Tokenisieren des gesamten Buches für jeden Chat zu vermeiden.Tiered Models: Nutzung günstiger Modelle (Haiku, Flash) für den "Background Reader", der den Graphen baut. Nutzung teurer Modelle (Opus, GPT-4) nur für den "Critic" oder "Drafter", wenn hohe Kreativität gefordert ist.3Local Dev: Optionale Anbindung an eine lokale LLM-Instanz (Ollama) für kostenloses, privates Drafting, synchronisiert mit dem "Cursor"-Modell der Privatsphäre.258.3 Kontext-Fenster-Management vs. RAGWährend "Long Context" mächtig ist, zeigt sich das "Lost in the Middle"-Phänomen.20 Ein hybrider Ansatz ist überlegen:Globale Abfragen ("Was ist das Thema?"): Nutzung von Hierarchischen Zusammenfassungen (Context Window).Spezifische Abfragen ("Was sagte Alice?"): Nutzung von RAG/GraphRAG.Schreibfortsetzung: Einspeisung der letzten 5k Wörter (Context Window) + abgerufene relevante Lore (RAG).9. FazitDie Entwicklung einer KI-gestützten Roman-Schreib-App auf Vercel ist eine anspruchsvolle ingenieurtechnische Herausforderung, die weit über einfache Chatbot-Wrapper hinausgeht. Durch die Adoption einer Architektur, die den Roman als strukturierte Datenbank behandelt (GraphRAG), Durable Execution (Inngest) für tiefe Analysen nutzt und agentische Workflows (LangGraph) für kritisches Reasoning einsetzt, lässt sich ein Werkzeug schaffen, das den autorschaftlichen Prozess wahrhaftig augmentiert. Dieses System generiert nicht einfach nur Text; es agiert als unermüdlicher Kontinuitäts-Editor, kreativer Partner und rigoroser Kritiker und erfüllt damit die Vision eines "Cursor für Romanautoren".KomponenteTechnologie-EmpfehlungBegründungFrontendNext.js (App Router) + shadcn/uiPerformance, Server Components, Polierte UIChat EngineVercel AI SDK CoreStreaming, Tool Calling, React IntegrationModell ProviderOpenRouterZugriff auf Claude 3.5 (Prosa) & GPT-4o (Logik)OrchestrierungLangGraph.jsZustandsbehaftete Zyklen, "Reflexion" LoopsHintergrund JobsInngestDurable Execution, vermeidet Serverless TimeoutsDatenbankPostgres (Text) + Neo4j (Graph)Relationale Integrität + Narrative BeziehungenValidierungZod + Constitutional AITypsicherheit + Leitplanken für KritikDieser Bericht liefert den Fahrplan, um vom Konzept zu einer produktionsreifen, skalierbaren Anwendung auf Vercel zu gelangen.
